{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU!\n"
     ]
    }
   ],
   "source": [
    "import dense_correspondence_manipulation.utils.utils as utils\n",
    "utils.add_dense_correspondence_to_python_path()\n",
    "import dense_correspondence\n",
    "reload(dense_correspondence)\n",
    "from dense_correspondence.training.training import *\n",
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "\n",
    "#utils.set_default_cuda_visible_devices()\n",
    "# utils.set_cuda_visible_devices([0]) # use this to manually set CUDA_VISIBLE_DEVICES\n",
    "\n",
    "import dense_correspondence.dataset.spartan_dataset_masked\n",
    "reload(dense_correspondence.dataset.spartan_dataset_masked)\n",
    "import dense_correspondence.training.training\n",
    "reload(dense_correspondence.training.training)\n",
    "from dense_correspondence.training.training import DenseCorrespondenceTraining\n",
    "from dense_correspondence.dataset.spartan_dataset_masked import SpartanDataset, OPDataSelector\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import dense_correspondence.evaluation.evaluation\n",
    "reload(dense_correspondence.evaluation.evaluation)\n",
    "from dense_correspondence.evaluation.evaluation import DenseCorrespondenceEvaluation\n",
    "\n",
    "# Multi Object Pursuit\n",
    "utils.add_object_pursuit_to_python_path()\n",
    "import object_pursuit\n",
    "reload(object_pursuit)\n",
    "from dense_correspondence.training.op_training import OPDenseCorrespondenceTraining\n",
    "\n",
    "from dense_correspondence.network.op_dense_correspondence_network import \\\n",
    "    OPMultiDenseCorrespondenceNetwork, OPMultiDenseCorrespondenceNetworkV2\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "DEVICE = \"cuda:1\" if cuda else \"cpu\"\n",
    "if cuda:\n",
    "    print(\"CUDA GPU!\")\n",
    "else:\n",
    "    print(\"CPU!\")\n",
    "    \n",
    "PRETRAINED_ROOT = \"/home/ashek/code/data/pdc/trained_models/tutorials\"\n",
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Finetuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose = False\n",
    "config_filename = os.path.join(utils.getDenseCorrespondenceSourceDir(), 'config', 'dense_correspondence', \n",
    "                               'dataset', 'composite', 'object_pursuit.yaml')\n",
    "data_config = utils.getDictFromYamlFilename(config_filename)\n",
    "data_config['logs_root_path'] = '/home/ashek/code/data/pdc/logs_proto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Pretrained objects: ', ['caterpillar', 'dramamine', 'drill', 'gopro_box', 'headphones'])\n",
      "{\n",
      "  \"device\": \"cuda:1\", \n",
      "  \"training\": {\n",
      "    \"data_type_probabilities\": {\n",
      "      \"DIFFERENT_OBJECT\": 0, \n",
      "      \"SINGLE_OBJECT_WITHIN_SCENE\": 1, \n",
      "      \"MULTI_OBJECT\": 0, \n",
      "      \"SYNTHETIC_MULTI_OBJECT\": 0, \n",
      "      \"SINGLE_OBJECT_ACROSS_SCENE\": 0\n",
      "    }, \n",
      "    \"num_test_imgs_per_scene\": 1, \n",
      "    \"save_rate\": 1000, \n",
      "    \"num_workers\": 5, \n",
      "    \"logging_dir\": \"/home/ashek/code/data/pdc/trained_models/tutorials\", \n",
      "    \"compute_test_loss\": true, \n",
      "    \"sample_matches_only_off_mask\": true, \n",
      "    \"fraction_masked_non_matches\": 0.5, \n",
      "    \"batch_size\": 1, \n",
      "    \"domain_randomize\": true, \n",
      "    \"num_matching_attempts\": 10000, \n",
      "    \"weight_decay\": 0.0001, \n",
      "    \"garbage_collect_rate\": 1, \n",
      "    \"compute_test_loss_rate\": 500, \n",
      "    \"num_non_matches_per_match\": 150, \n",
      "    \"num_iterations\": 3500, \n",
      "    \"steps_between_learning_rate_decay\": 250, \n",
      "    \"cross_scene_num_samples\": 10000, \n",
      "    \"logging_dir_name\": \"FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss\", \n",
      "    \"loss_function\": \"pixelwise_contrastive_loss\", \n",
      "    \"fraction_background_non_matches\": 0.5, \n",
      "    \"test_loss_num_iterations\": 36, \n",
      "    \"learning_rate_decay\": 0.9, \n",
      "    \"logging_rate\": 100, \n",
      "    \"learning_rate\": 0.0001, \n",
      "    \"use_image_b_mask_inv\": true\n",
      "  }, \n",
      "  \"dense_correspondence_network\": {\n",
      "    \"normalize\": false, \n",
      "    \"descriptor_dimension\": 3, \n",
      "    \"backbone\": {\n",
      "      \"resnet_name\": \"Resnet34_8s_OP\", \n",
      "      \"model_class\": \"Resnet\"\n",
      "    }, \n",
      "    \"image_width\": 640, \n",
      "    \"image_height\": 480, \n",
      "    \"device\": \"cuda:1\", \n",
      "    \"model_class\": \"<class 'dense_correspondence.network.op_dense_correspondence_network.OPMultiDenseCorrespondenceNetworkV2'>\", \n",
      "    \"OP\": {\n",
      "      \"freeze_backbone\": false, \n",
      "      \"class_num\": 5, \n",
      "      \"save_temp_interval\": 0, \n",
      "      \"model_type\": \"Multinet\", \n",
      "      \"use_backbone\": true, \n",
      "      \"z_dim\": 100, \n",
      "      \"express_threshold\": 0.13\n",
      "    }\n",
      "  }, \n",
      "  \"loss_function\": {\n",
      "    \"use_l2_pixel_loss_on_background_non_matches\": false, \n",
      "    \"M_masked\": 0.5, \n",
      "    \"M_background\": 0.5, \n",
      "    \"use_l2_pixel_loss_on_masked_non_matches\": false, \n",
      "    \"non_match_loss_weight\": 1.0, \n",
      "    \"alpha_triplet\": 0.1, \n",
      "    \"scale_by_hard_negatives\": true, \n",
      "    \"scale_by_hard_negatives_DIFFERENT_OBJECT\": true, \n",
      "    \"M_pixel\": 50, \n",
      "    \"match_loss_weight\": 1.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Pretrained model info\n",
    "model_name = \"op_multinet_pretraining_v2_unfrozen_standard_loss\"\n",
    "model_folder = os.path.join(PRETRAINED_ROOT, model_name)\n",
    "loaded_iter = 3501\n",
    "pretrained_model_path = os.path.join(model_folder, '%06d.pth' % loaded_iter)\n",
    "train_config = utils.getDictFromYamlFilename(os.path.join(model_folder, 'training.yaml'))\n",
    "pretrained_object_ids = list(np.load(os.path.join(model_folder, \"object_ids.npy\")))\n",
    "print(\"Pretrained objects: \", pretrained_object_ids)\n",
    "\n",
    "# NOTE: overwrite old log dir folder\n",
    "train_config['training']['logging_dir_name'] = \"FINETUNING_\" + model_name\n",
    "\n",
    "NetworkClass = OPMultiDenseCorrespondenceNetworkV2\n",
    "assert train_config['dense_correspondence_network']['model_class'] == str(NetworkClass)\n",
    "print(json.dumps(train_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Pursuit\n",
    "\n",
    "This should take about ~12-15 minutes with a GTX 1080 Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logs_root_path': '/home/ashek/code/data/pdc/logs_proto',\n",
       " 'multi_object_scenes_config_files': [],\n",
       " 'single_object_scenes_config_files': ['10_drill_scenes.yaml',\n",
       "  'caterpillar_upright.yaml',\n",
       "  'dramamine.yaml',\n",
       "  'gopro_box.yaml',\n",
       "  'headphones.yaml',\n",
       "  'mugs_brown.yaml',\n",
       "  'mugs_best.yaml',\n",
       "  'shoe_brown_boot.yaml',\n",
       "  'mugs_red.yaml',\n",
       "  'shoe_gray_nike.yaml',\n",
       "  'shoe_green_nike.yaml',\n",
       "  'shoe_red_nike.yaml',\n",
       "  'mugs_dreamy.yaml']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.get_pose_data(dataset.get_scene_list()[0])\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pretrained basis: headphones\n"
     ]
    }
   ],
   "source": [
    "z_idx = 4\n",
    "print(\"Using pretrained basis: %s\" % pretrained_object_ids[z_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SpartanDataset:\n",
      "   - in train mode\n",
      "   - number of scenes 64\n",
      "   - total images:     21859\n",
      "DATASET MODE: train\n",
      "Using SpartanDataset:\n",
      "   - in test mode\n",
      "   - number of scenes 27\n",
      "   - total images:     9753\n",
      "DATASET MODE: test\n",
      "using SINGLE_OBJECT_WITHIN_SCENE\n",
      "using SINGLE_OBJECT_WITHIN_SCENE\n",
      "using SINGLE_OBJECT_WITHIN_SCENE\n",
      "using SINGLE_OBJECT_WITHIN_SCENE\n",
      "('dir_name: ', 'FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss')\n",
      "('logging_dir:', '/home/ashek/code/data/pdc/trained_models/tutorials/FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss')\n",
      "ResNeet34 OP freeze backbone: False\n",
      "Starting pursuing:\n",
      "            z_dim:                            100\n",
      "            output dir:                       /home/ashek/code/data/pdc/trained_models/tutorials/FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss\n",
      "            device:                           cuda:1\n",
      "            pretrained_path:                  /home/ashek/code/data/pdc/trained_models/tutorials/op_multinet_pretraining_v2_unfrozen_standard_loss/003501.pth\n",
      "            bases dir:                        /home/ashek/code/data/pdc/trained_models/tutorials/FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss/Bases\n",
      "            express accuracy threshold:       0.13\n",
      "            save object interval:             0 (0 means don't save)\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashek/code/modules/dense_correspondence_manipulation/utils/utils.py:266: RuntimeWarning: invalid value encountered in arccos\n",
      "  theta = 2*np.arccos(2 * np.dot(q,r)**2 - 1)\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================start new object==============================\n",
      "Starting new object:\n",
      "                round:               1\n",
      "                current base num:    1\n",
      "                object data dir:     caterpillar\n",
      "                object index:        1\n",
      "                output obj dir:      /home/ashek/code/data/pdc/trained_models/tutorials/FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss/explored_objects/obj_1\n",
      "            \n",
      "Current object is novel, min loss: 10000000000.0 vs thresh 0.13, most similiar object: None, start object pursuit\n",
      "Z-acc pairs: []\n",
      "start coefficient pursuit (first check):\n",
      "coeff pursuit result dir: /home/ashek/code/data/pdc/trained_models/tutorials/FINETUNING_op_multinet_pretraining_v2_unfrozen_standard_loss/explored_objects/obj_1/coeff_pursuit\n",
      "Coeffnet: found 1 Base files\n",
      "Starting training:\n",
      "            net type:        {net_type}\n",
      "            Max epochs:      {max_epochs}\n",
      "            Batch size:      {batch_size}\n",
      "            Learning rate:   {lr}\n",
      "            Checkpoints:     {save_cp_path}\n",
      "            z_dir:           {z_dir}\n",
      "            wait epochs:     {wait_epochs}\n",
      "            trainable parameter number of the primarynet: {num_trainable_primary_net_params}\n",
      "            trainable parameter number of the hypernet: {num_trainable_hypernet_params}\n",
      "        \n",
      "Start epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/80:   0%|          | 0/64 [00:00<?, ?img/s]/home/ashek/.local/lib/python2.7/site-packages/torch/nn/functional.py:2589: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n",
      "Epoch 1/80:   2%|▏         | 1/64 [00:05<06:09,  5.87s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/80:   2%|▏         | 1/64 [00:03<03:37,  3.45s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/80:   2%|▏         | 1/64 [00:03<03:37,  3.45s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/80:   2%|▏         | 1/64 [00:03<03:37,  3.45s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/80:   2%|▏         | 1/64 [00:03<03:37,  3.46s/img]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/80:   0%|          | 0/64 [00:03<?, ?img/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8fbabfc14566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPDenseCorrespondenceTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetworkClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNetworkClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"finished training descriptor of dimension %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/training/op_training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, pretrained_model_path, loss_current_iteration, z_idx)\u001b[0m\n\u001b[1;32m    523\u001b[0m                                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                                                          \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                                                          l1_loss_coeff=0.2)\n\u001b[0m\u001b[1;32m    526\u001b[0m                 self.write_log(\n\u001b[1;32m    527\u001b[0m                     str(f(\"training stop, min_val_loss: {min_val_loss}\")))\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/training/op_training.py\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(self, samples, base_num, net_type, hypernet, backbone, zs, save_cp_path, max_epochs, batch_size, lr, wait_epochs, l1_loss_coeff, mem_loss_coeff)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     (loss, match_loss, masked_non_match_loss,\n\u001b[0;32m--> 273\u001b[0;31m                         background_non_match_loss, blind_non_match_loss) = self.calc_losses(primary_net, self._batchify_sample(sample))\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/training/op_training.py\u001b[0m in \u001b[0;36mcalc_losses\u001b[0;34m(self, dcn, sample)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Don't index with any particular object index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# either SingleNet using one z basis or CoeffNet using all z basis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDenseCorrespondenceTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetup_logging_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/training/training.pyc\u001b[0m in \u001b[0;36mcalc_losses\u001b[0;34m(self, dcn, sample)\u001b[0m\n\u001b[1;32m    266\u001b[0m         return loss_composer.get_loss(self.loss_fn, match_type,\n\u001b[1;32m    267\u001b[0m                                       \u001b[0mimage_a_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_b_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                                       masked_non_matches_a, masked_non_matches_b, background_non_matches_a, background_non_matches_b, blind_non_matches_a, blind_non_matches_b)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/loss_functions/loss_composer.pyc\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(pixelwise_contrastive_loss, match_type, image_a_pred, image_b_pred, matches_a, matches_b, masked_non_matches_a, masked_non_matches_b, background_non_matches_a, background_non_matches_b, blind_non_matches_a, blind_non_matches_b)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                      \u001b[0mmasked_non_matches_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_non_matches_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                      \u001b[0mbackground_non_matches_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_non_matches_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                      blind_non_matches_a, blind_non_matches_b)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmatch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSpartanDatasetDataType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSINGLE_OBJECT_ACROSS_SCENE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/loss_functions/loss_composer.pyc\u001b[0m in \u001b[0;36mget_within_scene_loss\u001b[0;34m(pixelwise_contrastive_loss, image_a_pred, image_b_pred, matches_a, matches_b, masked_non_matches_a, masked_non_matches_b, background_non_matches_a, background_non_matches_b, blind_non_matches_a, blind_non_matches_b)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_non_match_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_masked_hard_negatives\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         pixelwise_contrastive_loss.get_loss_matched_and_non_matched_with_l2(\n\u001b[0;32m---> 83\u001b[0;31m             image_a_pred, image_b_pred, matches_a, matches_b, masked_non_matches_a, masked_non_matches_b,  M_descriptor=pcl._config[\"M_masked\"])\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_l2_pixel_loss_on_background_non_matches\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/loss_functions/pixelwise_contrastive_loss.pyc\u001b[0m in \u001b[0;36mget_loss_matched_and_non_matched_with_l2\u001b[0;34m(self, image_a_pred, image_b_pred, matches_a, matches_b, non_matches_a, non_matches_b, M_descriptor, M_pixel, non_match_loss_weight, use_l2_pixel_loss)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# version with no l2 pixel term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             non_match_loss, num_hard_negatives = self.non_match_loss_descriptor_only(\n\u001b[0;32m---> 94\u001b[0;31m                 image_a_pred, image_b_pred, non_matches_a, non_matches_b, M_descriptor=M_descriptor)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_match_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hard_negatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/loss_functions/pixelwise_contrastive_loss.pyc\u001b[0m in \u001b[0;36mnon_match_loss_descriptor_only\u001b[0;34m(self, image_a_pred, image_b_pred, non_matches_a, non_matches_b, M_descriptor, invert)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         non_match_loss_vec, num_hard_negatives, _, _ = PCL.non_match_descriptor_loss(image_a_pred, image_b_pred, non_matches_a,\n\u001b[0;32m--> 309\u001b[0;31m                                                                                      non_matches_b, M=M_descriptor, invert=invert)\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mnum_non_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_match_loss_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ashek/code/dense_correspondence/loss_functions/pixelwise_contrastive_loss.pyc\u001b[0m in \u001b[0;36mnon_match_descriptor_loss\u001b[0;34m(image_a_pred, image_b_pred, non_matches_a, non_matches_b, M, invert)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mnon_match_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_match_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mhard_negative_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_match_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mnum_hard_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhard_negative_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = OPDenseCorrespondenceTraining(NetworkClass=NetworkClass, config=train_config)\n",
    "train.load_dataset_from_config(config=data_config)\n",
    "train.run(pretrained_model_path=pretrained_model_path, z_idx=z_idx)\n",
    "print \"finished training descriptor of dimension %d\" %(d)\n",
    "\n",
    "# Each batch, select a random image from a random scene\n",
    "# for that image, find another image from the same scene\n",
    "# but a taken from a diff view.\n",
    "# then generate both pixel matches and non-matches \n",
    "# yes there is a heuristic approach, but it seeems complex\n",
    "# and thus runtime is long. Prune non-matches based on:\n",
    "# 1. depth is 0 (thus invalid, cannot use)\n",
    "# 2. Project pixels from im1 to onto im2 and check if outside FOV\n",
    "# 3. check if pixel is occluded using some method\n",
    "# complicated... don't touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
